{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> está compuesto por las siguientes variables referidas siempre al año 2018:\n",
    "\n",
    "1. **Month** 1-4\n",
    "2. **DayofMonth** 1-31\n",
    "3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "4. **FlightDate** fecha del vuelo\n",
    "5. **Origin** código IATA del aeropuerto de origen\n",
    "6. **OriginCity** ciudad donde está el aeropuerto de origen\n",
    "7. **Dest** código IATA del aeropuerto de destino\n",
    "8. **DestCity** ciudad donde está el aeropuerto de destino  \n",
    "9. **DepTime** hora real de salida (local, hhmm)\n",
    "10. **DepDelay** retraso a la salida, en minutos\n",
    "11. **ArrTime** hora real de llegada (local, hhmm)\n",
    "12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterrizó menos de 15 minutos más tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n",
    "13. **Cancelled** si el vuelo fue cancelado (1 = sí, 0 = no)\n",
    "14. **CancellationCode** razón de cancelación (A = aparato, B = tiempo atmosférico, C = NAS, D = seguridad)\n",
    "15. **Diverted** si el vuelo ha sido desviado (1 = sí, 0 = no)\n",
    "16. **ActualElapsedTime** tiempo real invertido en el vuelo\n",
    "17. **AirTime** en minutos\n",
    "18. **Distance** en millas\n",
    "19. **CarrierDelay** en minutos: El retraso del transportista está bajo el control del transportista aéreo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, daño de la aeronave, espera de la llegada de los pasajeros o la tripulación de conexión, equipaje, impacto de un pájaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulación (descanso del piloto o acompañante) , daños por mercancías peligrosas, inspección de ingeniería, abastecimiento de combustible, pasajeros discapacitados, tripulación retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegación de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no válido, retrasos de peso y equilibrio.\n",
    "20. **WeatherDelay** en minutos: causado por condiciones atmosféricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n",
    "21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorológicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tráfico aéreo, problemas con los controladores aéreos, etc.\n",
    "22. **SecurityDelay** en minutos: causado por la evacuación de una terminal, re-embarque de un avión debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n",
    "23. **LateAircraftDelay** en minutos: debido al propio retraso del avión al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora más tardía de la que estaba prevista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera línea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opción header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto no hace nada: la lectura es lazy así que no se lee en realidad hasta que ejecutemos una acción sobre flightsDF\n",
    "# Solamente se comprueba que exista el fichero en esa ruta, y se leen los nombres de columnas\n",
    "flightsDF = spark.read.option(\"header\", \"true\").csv(\"C:/Users/nerea.gomez/Documents/Documentacion/Ejercicios DataFrames/flights-jan-apr-2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos el esquema (nombre y tipo de dato de cada columna). Esto son solamente metadatos, por lo que no es ninguna acción.\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las columnas son cadenas de caracteres porque no hemos indicado el tipo de dato para cada columna ni tampoco le hemos pedido a Spark que intente inferirlo a partir de los datos. No queremos que todas sean string porque hay algunas numéricas que deberían ser tratadas como tales. Vamos a intentar inferir el esquema. Esto supone una lectura un poco más lenta y también es más lento que una tercera opción que consiste en indicar explícitamente el esquema para los datos en el momento de la lectura, que es la opción recomendada si sabemos de antemano qué tipo va a tener cada columna. Si lo hiciésemos de esa manera, en caso de que no se pueda leer con ese esquema obtendríamos un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "flightsDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"C:/Users/nerea.gomez/Documents/Documentacion/Ejercicios DataFrames/flights-jan-apr-2018.csv\") # pon aquí la ruta en tu bucket\n",
    "\n",
    "# Ensuciamos a propósito la variable ArrDelay para que pase a ser un string como suele pasar con frecuencia\n",
    "flightsDF = flightsDF.withColumn(\"ArrDelay\", F.when(F.rand(seed = 123) < 0.1, \"NA\").otherwise(F.col(\"ArrDelay\")))\n",
    "\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tiene mejor pinta, aunque todavía hay algunas columnas cuyo tipo de dato sigue siendo string cuando la intuición nos dice que deberían ser enteros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas con Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contamos el número de filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una de las primeras cosas que nos preguntamos sobre un dataset: número de filas y columnas (cuántos ejemplos y cuántas variables tenemos para describirlos). Puesto que vamos a llevar a cabo varias transformaciones al DataFrame `flightsDF` a partir de este punto, vamos a usar `cache`() para que Spark lo mantenga en memoria en lugar de liberar la memoria ocupada tras cada acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los datos tienen 23 columnas\n",
      "Los datos tienen 2503113 filas\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los nombres de columna. Esto son solo metadatos de DataFrame, y están en el driver. No es necesaria ninguna\n",
    "# operación sobre el cluster para recuperar la variable interna columns de cualquier DataFrame\n",
    "print(\"Los datos tienen {0} columnas\".format(len(flightsDF.columns)))\n",
    "\n",
    "flightsDF.cache()        # Esta línea no hace cálculos, pero Spark anota que debe mantener este DF en memoria tras la primera vez que sea materializado\n",
    "rows = flightsDF.count() # Esto es una acción que obligará a que flightsDF sea materializado. Para ello, habrá que llevar a cabo\n",
    "                         # las transformaciones que lo generan en la celda anterior: read y withColumn, que están pendientes\n",
    "\n",
    "print(\"Los datos tienen {0} filas\".format(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar columnas por nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|Month|DayofMonth|ArrTime|\n",
      "+-----+----------+-------+\n",
      "|    1|        14|   null|\n",
      "|    1|         3|   1506|\n",
      "|    1|         6|   1543|\n",
      "|    1|         7|   1455|\n",
      "|    1|         8|   1509|\n",
      "|    1|         9|   1504|\n",
      "|    1|        10|   1455|\n",
      "|    1|        11|   1452|\n",
      "|    1|        12|   1748|\n",
      "|    1|        13|   1514|\n",
      "|    1|        15|   1456|\n",
      "|    1|        16|   1511|\n",
      "|    1|        17|   1622|\n",
      "|    1|        18|   1509|\n",
      "|    1|        19|   1449|\n",
      "|    1|        20|   1533|\n",
      "|    1|        21|   1508|\n",
      "|    1|        22|   1504|\n",
      "|    1|        23|   1616|\n",
      "|    1|        24|   1515|\n",
      "+-----+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.select(\"Month\", \"DayofMonth\", \"ArrTime\")\\\n",
    "         .show() # los nombres son sensibles a mayúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtramos (retenemos) filas en base a los valores de una o varias columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de las transformaciones están definidas en el paquete `pyspark.sql.functions`, por lo que es frecuente importar el paquete completo con un alias, como `F`, en lugar de importar cada función individual. A partir de ese momento usamos `F.` antes del nombre de cada función, para decirle a python dónde buscar esa función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hubo 16176 vuelos el 20 de enero de 2018\n",
      "+-----+-------+\n",
      "|Month|ArrTime|\n",
      "+-----+-------+\n",
      "|    1|   1533|\n",
      "|    1|   1734|\n",
      "|    1|   2025|\n",
      "+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La función col se utiliza para decir que nos estamos refiriendo a la columna cuyo nombre se pasa como argumento\n",
    "\n",
    "flightsJanuary20 = flightsDF\\\n",
    "                      .where((F.col(\"DayofMonth\") == 20) & (F.col(\"Month\") == 1))\\\n",
    "                      .select(\"Month\", \"ArrTime\") # encadenamos dos transformaciones: esto no desencadena ninguna operación\n",
    "\n",
    "# Cúantos vuelos hay el 20 de enero de 2018\n",
    "# La operación count() es una acción, así que obliga a materializar flightsJanuary20. Para ello es necesario ejecutar\n",
    "# las transformaciones where() y select() que pusimos en esta celda, aplicadas a flightsDF. De hecho, si no hubiésemos\n",
    "# cacheado flightsDF en las celdas anteriores, también habría que materializarlo otra vez, y para eso se leería de \n",
    "# nuevo el CSV desde HDFS\n",
    "rowsJanuary20 = flightsJanuary20.count()\n",
    "\n",
    "print(\"Hubo {0} vuelos el 20 de enero de 2018\".format(rowsJanuary20))\n",
    "\n",
    "# Esto es otra acción aplicada sobre el DataFrame flightsJanuary. Como flightsJanuary NO ha sido cacheado,\n",
    "# entonces las operaciones \"where\" y \"select\" se necesitan ejecutar DE NUEVO para poder hacer el show()\n",
    "flightsJanuary20.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No olvidemos cachear el DataFrame cuando tengamos previsto hacer varias operaciones sobre él, o de lo contrario estaremos repitiendo muchas veces los cálculos previos que llevaron a ese DataFrame!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20519"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# También podemos indicar el filtrado como un string con un trozo de código SQL\n",
    "# Recordemos que where y filter son exactamente equivalentes\n",
    "flightsJanuary31 = flightsDF.filter(\"DayofMonth = 31 and Month = 1\") # transformación filter: Spark no ejecuta nada\n",
    "\n",
    "flightsJanuary31.count() # acción count: obliga a materializar flightsJanuary31 para lo cual se tiene que ejecutar filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>RECUERDA:</b> Spark hace todas estas operaciones de manera distribuida en el cluster, por tanto cada nodo del cluster está filtrando filas de entre aquellas que están presentes en ese nodo (más precisamente, en ese executor). Cada executor envía al driver el recuento de cuántas filas ha filtrado, y los resultados son agregados en el driver para mostrar el recuento total.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: muestra por pantalla el retraso en la llegada, el aeropuerto de origen y de destino de aquellos vuelos que tuvieron lugar en Domingo y con un retraso a la llegada mayor de 15 minutos. Muestra el esquema de dicho DataFrame resultante.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsSunday= flightsDF.select(F.col(\"ArrDelay\").alias(\"Retraso\"), F.col(\"OriginCity\").alias(\"Origen\"),\n",
    "                                F.col(\"DestCity\").alias(\"Destino\"))\\\n",
    "                        .filter(\"DayOfWeek=7 and ArrDelay>15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+\n",
      "|Retraso|         Origen|            Destino|\n",
      "+-------+---------------+-------------------+\n",
      "|   21.0|     Boston, MA|        Buffalo, NY|\n",
      "|   16.0|     Boston, MA|       New York, NY|\n",
      "|   22.0|   New York, NY|        Madison, WI|\n",
      "|  136.0|     Boston, MA|       New York, NY|\n",
      "|  119.0|    Detroit, MI|       New York, NY|\n",
      "|   90.0| Cincinnati, OH|        Detroit, MI|\n",
      "|   61.0|Gainesville, FL|        Atlanta, GA|\n",
      "|   48.0|    Atlanta, GA|     Shreveport, LA|\n",
      "|  158.0|Minneapolis, MN|         Newark, NJ|\n",
      "|   17.0|  Las Vegas, NV|Fort Lauderdale, FL|\n",
      "+-------+---------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsSunday.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Retraso: string (nullable = true)\n",
      " |-- Origen: string (nullable = true)\n",
      " |-- Destino: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsSunday.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar filas únicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacerse mejor idea de cómo es una variable categórica, es lógico querer cuántos valores distintos existen en nuestro dataset. Si consideramos todas las columnas, sería raro tener dos filas exactamente iguales en todos los valores, pero si seleccionamos solamente una o unas pocas columnas, podemos ver cuántas combinaciones distintas de valores de esas columnas se dan en el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 2483849 filas distintas\n"
     ]
    }
   ],
   "source": [
    "distinctFlights = flightsDF.distinct()  # distinct es una transformación que devuelve el DF sin las filas repetidas\n",
    "distinctFlightsCount = distinctFlights.count() # count es una acción y provoca que se ejecute la transformación distinct\n",
    "\n",
    "print(\"Hay {0} filas distintas\".format(distinctFlightsCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si seleccionamos solo las columnas `Origin` y `Dest` y nos quedamos con las filas distintas (quitamos repetidos), entonces obtenemos un DataFrame con los posibles aeropuertos de origen y destino, es decir, aquellos trayectos que existen en un vuelo (pueden aparecer varias veces porque seguramente existirán muchos vuelos a lo largo de 2018 entre un origen y un destino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>TU TURNO</b>: ¿<b>cuántas</b> combinaciones de Origin y Dest existen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 5352 combinaciones distintas\n"
     ]
    }
   ],
   "source": [
    "flights_dist_od= flightsDF.select(\"Origin\", \"Dest\")\\\n",
    "                          . distinct()\n",
    "flights_dist_od_count= flights_dist_od.count()\n",
    "\n",
    "print(\"Hay {0} combinaciones distintas\".format(flights_dist_od_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta en la siguiente celda:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 5352 combinaciones de un aeropuerto de origen y uno de destino\n"
     ]
    }
   ],
   "source": [
    "countOriginDests = flightsDF.select(\"OriginCity\", \"DestCity\")\\\n",
    "                          . distinct().count()\n",
    "\n",
    "print(\"Hay {0} combinaciones de un aeropuerto de origen y uno de destino\".format(countOriginDests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque tengamos 2.5 millones de filas, solo hay 5795 combinaciones distintas de un aeropuerto de origen y otro de destino.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>TU TURNO</b>: ¿<b>cuántos</b> aeropuertos de origen existen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 350 aeropuertos desde los que puede partir un vuelo\n"
     ]
    }
   ],
   "source": [
    "distinctOrigins = flightsDF.select(\"Origin\")\\\n",
    "                          .distinct().count()\n",
    "\n",
    "print(\"Hay {0} aeropuertos desde los que puede partir un vuelo\".format(distinctOrigins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: vamos a hacer esto para cada columna, en un bucle, para hacernos a la idea de cuántos valores hay. Esto solo tiene sentido en realidad para columnas categóricas y no para numéricas donde casi todos los valores serán distintos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No lo he sacado sola**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Month|DayofMonth|DayOfWeek|FlightDate|Origin|OriginCity|Dest|DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|    4|        31|        7|       120|   356|       350| 356|     350|   1440|    1378|   1440|    1369|        2|               5|       2|              705|    669|    1466|        1094|         748|     609|          128|              739|\n",
      "+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dfConteos(sparkDF):\n",
    "    colsRecuento = [F.countDistinct(c).alias(c) for c in sparkDF.columns] ##Cuenta los valores distintos de la columna\n",
    "    return sparkDF.select(colsRecuento)\n",
    "\n",
    "conteosDF = dfConteos(flightsDF)\n",
    "conteosDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 4 valores distintos en la columna Month\n",
      "Existen 31 valores distintos en la columna DayofMonth\n",
      "Existen 7 valores distintos en la columna DayOfWeek\n",
      "Existen 120 valores distintos en la columna FlightDate\n",
      "Existen 356 valores distintos en la columna Origin\n",
      "Existen 350 valores distintos en la columna OriginCity\n",
      "Existen 356 valores distintos en la columna Dest\n",
      "Existen 350 valores distintos en la columna DestCity\n",
      "Existen 1441 valores distintos en la columna DepTime\n",
      "Existen 1379 valores distintos en la columna DepDelay\n",
      "Existen 1441 valores distintos en la columna ArrTime\n",
      "Existen 1370 valores distintos en la columna ArrDelay\n",
      "Existen 2 valores distintos en la columna Cancelled\n",
      "Existen 6 valores distintos en la columna CancellationCode\n",
      "Existen 2 valores distintos en la columna Diverted\n",
      "Existen 706 valores distintos en la columna ActualElapsedTime\n",
      "Existen 670 valores distintos en la columna AirTime\n",
      "Existen 1466 valores distintos en la columna Distance\n",
      "Existen 1095 valores distintos en la columna CarrierDelay\n",
      "Existen 749 valores distintos en la columna WeatherDelay\n",
      "Existen 610 valores distintos en la columna NASDelay\n",
      "Existen 129 valores distintos en la columna SecurityDelay\n",
      "Existen 740 valores distintos en la columna LateAircraftDelay\n"
     ]
    }
   ],
   "source": [
    "for columnName in flightsDF.columns:\n",
    "\n",
    "    distinctValues = flightsDF.select(columnName).distinct().count()##aplica la funcion al DF \n",
    "    \n",
    "    # No olvidéis indentar este comando para indicar que está dentro del cuerpo del bucle\n",
    "    print(\"Existen {0} valores distintos en la columna {1}\".format(distinctValues, columnName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in flightsDF.columns:\n",
    "    distinctValues = flightsDF.select(x).distinct().count()\n",
    "    columnName = x\n",
    "    # No olvidéis indentar este comando para indicar que está dentro del cuerpo del bucle\n",
    "    print(\"Existen {0} valores distintos en la columna {1}\".format(distinctValues, columnName))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear una nueva columna o reemplazar una existente por el resultado de operar con columnas existentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las distancias en millas. Vamos a convertirlas a km multiplicando las millas por 1.61. De nuevo, cada nodo del cluster hará esta operación localmente con los datos que se encuentran en ese nodo. El DataFrame resultante estará repartido en los mismos nodos. **No hay movimiento de datos ya que no se necesita ninguna información que no esté presente en ese nodo** para efectuar la operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- DistanceKm: double (nullable = true)\n",
      "\n",
      "+-----+----------+---------+----------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n",
      "|Month|DayofMonth|DayOfWeek|FlightDate|Origin|  OriginCity|Dest|    DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|        DistanceKm|\n",
      "+-----+----------+---------+----------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n",
      "|    1|        14|        7|2018-01-14|   SYR|Syracuse, NY| DTW| Detroit, MI|   null|    null|   null|    null|      1.0|               B|     0.0|             null|   null|   374.0|        null|        null|    null|         null|             null|            602.14|\n",
      "|    1|         3|        3|2018-01-03|   SYR|Syracuse, NY| LGA|New York, NY|   1348|   -10.0|   1506|   -13.0|      0.0|            null|     0.0|             78.0|   42.0|   198.0|        null|        null|    null|         null|             null|318.78000000000003|\n",
      "|    1|         6|        6|2018-01-06|   SYR|Syracuse, NY| LGA|New York, NY|   1410|    12.0|   1543|    24.0|      0.0|            null|     0.0|             93.0|   45.0|   198.0|        12.0|         0.0|    12.0|          0.0|              0.0|318.78000000000003|\n",
      "+-----+----------+---------+----------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn is a transformation returing a new DataFrame with one extra column appended on the right\n",
    "flightsWithKm = flightsDF.withColumn(\"DistanceKm\", F.col(\"Distance\") * 1.61)\n",
    "\n",
    "flightsWithKm.printSchema()\n",
    "\n",
    "flightsWithKm.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El día de la semana es una variable entera. Utilizar un entero o una variable categórica es una decisión que depende de qué tipo de modelo vayamos a ajustar. ¿Tiene sentido considerar días de la semana \"más grandes\" o \"más pequeños\", es decir, algo que se incrementa conforme \"se incrementa\" el día de la semana de 1 a 7? Aquí vamos a **reemplazar** la columna `DayOfWeek` que ya existía, por una versión categórica como strings. Utilizamos `withColumn` pasándole como primer argumento el nombre de una columna que ya existe, lo cual indica que queremos reemplazarla, en el mismo lugar que ocupaba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = false)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n",
      "+---------+-------+-------+\n",
      "|DayOfWeek|DepTime|ArrTime|\n",
      "+---------+-------+-------+\n",
      "|   Sunday|   null|   null|\n",
      "|Wednesday|   1348|   1506|\n",
      "| Saturday|   1410|   1543|\n",
      "|   Sunday|   1347|   1455|\n",
      "|   Monday|   1350|   1509|\n",
      "|  Tuesday|   1351|   1504|\n",
      "|Wednesday|   1347|   1455|\n",
      "| Thursday|   1345|   1452|\n",
      "|   Friday|   1640|   1748|\n",
      "| Saturday|   1338|   1514|\n",
      "|   Monday|   1353|   1456|\n",
      "|  Tuesday|   1357|   1511|\n",
      "|Wednesday|   1526|   1622|\n",
      "| Thursday|   1354|   1509|\n",
      "|   Friday|   1344|   1449|\n",
      "| Saturday|   1433|   1533|\n",
      "|   Sunday|   1353|   1508|\n",
      "|   Monday|   1354|   1504|\n",
      "|  Tuesday|   1501|   1616|\n",
      "|Wednesday|   1355|   1515|\n",
      "+---------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsCategoricalDay = flightsDF.withColumn(\"DayOfWeek\", F.when(F.col(\"DayOfWeek\") == 1, \"Monday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 2, \"Tuesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 3, \"Wednesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 4, \"Thursday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 5, \"Friday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 6, \"Saturday\")\\\n",
    "                                                           .otherwise(\"Sunday\"))\n",
    "\n",
    "flightsCategoricalDay.printSchema() # the column is still in the same position but has now string type\n",
    "\n",
    "flightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>CONSEJO:</b> El proceso de crear nuevas variables o <i>features</i> a partir de otras existentes, incluso incorporar variables de fuentes de datos externas (datos públicos) y de limpiar o reemplazar variables tras normalizarla se denomina genéricamente <i>feature engineering</i> (ingeniería de variables). A veces tiene mucho que ver con conocimientos específicos del dominio, aunque también con trucos estadísticos para normalizar siguiend métodos bien conocidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `when` es muy común para re-categorizar una variable o para crear nuevas variables categóricas a partir de condiciones complejas acerca de lo que les ocurre a los valores de otras columnas en esa misma fila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: crea una variable categórica de tipo string con dos categorías indicando si el día de la semana es laborable o es fin de semana. Los valores deben ser \"laborable\" o \"finde\". Utiliza `withColumn` y `when`. Será laborable cuando DayOfWeek esté entre 1 y 5, y fin de semana cuando sea 6 o 7 (en resumen: \"en otro caso\" es fin de semana). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- Laboral_finde: string (nullable = false)\n",
      "\n",
      "+---------+-------+-------+-------------+\n",
      "|DayOfWeek|DepTime|ArrTime|Laboral_finde|\n",
      "+---------+-------+-------+-------------+\n",
      "|        7|   null|   null|        finde|\n",
      "|        3|   1348|   1506|    laborable|\n",
      "|        6|   1410|   1543|        finde|\n",
      "|        7|   1347|   1455|        finde|\n",
      "|        1|   1350|   1509|    laborable|\n",
      "|        2|   1351|   1504|    laborable|\n",
      "|        3|   1347|   1455|    laborable|\n",
      "|        4|   1345|   1452|    laborable|\n",
      "|        5|   1640|   1748|    laborable|\n",
      "|        6|   1338|   1514|        finde|\n",
      "|        1|   1353|   1456|    laborable|\n",
      "|        2|   1357|   1511|    laborable|\n",
      "|        3|   1526|   1622|    laborable|\n",
      "|        4|   1354|   1509|    laborable|\n",
      "|        5|   1344|   1449|    laborable|\n",
      "|        6|   1433|   1533|        finde|\n",
      "|        7|   1353|   1508|        finde|\n",
      "|        1|   1354|   1504|    laborable|\n",
      "|        2|   1501|   1616|    laborable|\n",
      "|        3|   1355|   1515|    laborable|\n",
      "+---------+-------+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsCategoricalDay = flightsDF.withColumn(\"Laboral_finde\", F.when(F.col(\"DayOfWeek\") <= 5, \"laborable\")\\\n",
    "                                                           .otherwise(\"finde\"))\n",
    "flightsCategoricalDay.printSchema()\n",
    "flightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\",\"Laboral_finde\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear y seleccionar columnas al vuelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`withColumn` no es la única manera de crear columnas. Podemos usar `select` no solo para seleccionar columnas existentes sino también para crear nuevas columnas al vuelo y a la vez seleccionarlas. Vamos a seleccionar Origin y Dest, que ya existen, a la vez que seleccionamos una tercera columna que estamos creando al vuelo, con la transformación de la distancia a km. Le ponemos como nombre \"DistanceKm\" en ese momento que la estamos creando, mediante la función `alias`. Si no usamos `alias`, Spark le pondrá a la nueva columna un nombre por defecto que en este caso sería \"1.6 * Distance\". Se recomienda usar alias para dar nombre a las nuevas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|Origin|Dest|DistanceKm|\n",
      "+------+----+----------+\n",
      "|   SYR| DTW|     598.4|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "+------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.selectExpr(\"Origin\", \"Dest\", \"1.6*Distance as DistanceKm\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|Origin|Dest|DistanceKm|\n",
      "+------+----+----------+\n",
      "|   SYR| DTW|     598.4|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "|   SYR| LGA|     316.8|\n",
      "+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsAirportsAndKm = flightsDF.select(F.col(\"Origin\"),\\\n",
    "                                        F.col(\"Dest\"),\\\n",
    "                                        (1.6 * F.col(\"Distance\")).alias(\"DistanceKm\"))\n",
    "\n",
    "flightsAirportsAndKm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TU TURNO</b>: crear un DataFrame con tres columnas seleccionando \"Origin\", \"OriginCity\" y una nueva columna de tipo string creada concatenando Origin y OriginCity con un guión \"-\". Utilizar `withColumn` y dentro la función `concat_ws` (concatenar con separador) con sintaxis <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat_ws\">F.concat_ws(\"-\", columna1, columna2)</a> del paquete pyspark.sql.functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsConcat = flightsDF.withColumn(\"Concat\",F.concat_ws(\"-\",\"Origin\",\"OriginCity\"))\\\n",
    "                                     .select(\"Origin\",\"OriginCity\", \"Concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------------+\n",
      "|Origin|  OriginCity|         Origin2|\n",
      "+------+------------+----------------+\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n",
      "+------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fightsOrigin = flightsDF.select(\"Origin\", \"OriginCity\", F.concat_ws(\"-\",\"Origin\",\"OriginCity\").alias(\"Origin2\"))\n",
    "fightsOrigin.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir el tipo de dato de una columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con frecuencia, Spark no infiere correctamente el tipo de cada columna. \n",
    "* A veces ocurre que una columna numérica no se reconozca adecuadamente porque los datos tienen \"NA\" (un string) que quería significar data faltante en el dataset original. \"NA\" no es un string especial para Spark, por lo que simplemente reconoce que la columna tiene tanto enteros como strings, y el tipo más general que infiere es string para esa columna. \n",
    "* Pero no es el caso aquí porque no se da esto, y por eso hemos ensuciado al principio los datos a propósito. Vamos a limpiarlos y a convertir la columna `ArrDelay` a DoubleType como debe ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay  250324 filas con NA en ArrDelay\n"
     ]
    }
   ],
   "source": [
    "naCount = flightsDF.where(\"ArrDelay = 'NA'\").count()\n",
    "\n",
    "# Esto es sintaxis SQL, pero también podríamos haberla llamado como .where(F.col(\"ArrDelay\") == \"NA\"). Ambas son equivalentes.\n",
    "\n",
    "print(\"Hay \", naCount, \"filas con NA en ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196188"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.where(F.col(\"ArrDelay\") != \"NA\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hay \"pocos\" nulos(11.39%), eliminamos las filas que los contienen y nos quedamos un un DF mas \"pequeño\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.398113458410664"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "250324*100/2196188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "flightsDF = flightsDF.where((F.col(\"ArrDelay\") != \"NA\"))\\\n",
    "                     .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenación respecto a una columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una transformación que nos devuelve otro DataFrame ordenado según la(s) columna(s) indicada(s), sea de forma ascendente o descendente. El DataFrame original no se modifica (al igual que ocurre en cualquier transformación). Se puede ordenar en base a una columna numérica (lo más frecuente) o también categórica (los strings se ordenan alfabéticamente). Para ordenar sí es necesario enviar información de unos nodos a otros puesto que no sabemos si existen o no datos mayores o menos que el valor que tenemos en el nodo (o más precisamente, en el worker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+\n",
      "|ArrDelay|Origin|Dest|\n",
      "+--------+------+----+\n",
      "|  2454.0|   PPG| HNL|\n",
      "|  1778.0|   ORF| DFW|\n",
      "|  1757.0|   SMF| DFW|\n",
      "|  1717.0|   HNL| JFK|\n",
      "|  1704.0|   EGE| DFW|\n",
      "|  1685.0|   ORF| JFK|\n",
      "|  1648.0|   SLC| DFW|\n",
      "|  1576.0|   IAH| MIA|\n",
      "|  1558.0|   DAY| DFW|\n",
      "|  1554.0|   EGE| MIA|\n",
      "+--------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenamos los vuelos según ArrDelay\n",
    "sortedDF = flightsDF.orderBy(\"ArrDelay\")  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n",
    "\n",
    "sortedDescDF = flightsDF.orderBy(\"ArrDelay\", ascending = False)\n",
    "sortedDescDF.select(\"ArrDelay\", \"Origin\", \"Dest\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de agregación sobre el DataFrame completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene implementaciones distribuidas y paralelas de funciones de agregación frecuentes como la media, min, max y desviación típica entre otras. Todas estas funciones reciben como argumento el objeto columna sobre el que la función debe aplicarse. Lo más habitual es aplicarlas para agregar por grupos, pero esto lo veremos en el segundo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a seleccionar vuelos con `ArrDelay` mayor de 15 minutos, y con ellos vamos a crear columnas con el min, max, media y desviación típica de la columna `ArrDelay`. Crearemos y seleccionaremos dichas columnas al vuelo, como vimos en la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196188"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+----------------+\n",
      "|     MeanArrDelay|MinArrDelay|MaxArrDelay|  StddevArrDelay|\n",
      "+-----------------+-----------+-----------+----------------+\n",
      "|65.08545785536556|       16.0|     2454.0|82.6537213783575|\n",
      "+-----------------+-----------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we select those flights with at least 16 minutes of delay and then compute the aggregations\n",
    "flightsDF.where(F.col(\"ArrDelay\") > 15)\\\n",
    "         .select(F.mean(\"ArrDelay\").alias(\"MeanArrDelay\"),\\\n",
    "                 F.min(\"ArrDelay\").alias(\"MinArrDelay\"),\\\n",
    "                 F.max(\"ArrDelay\").alias(\"MaxArrDelay\"),\n",
    "                 F.stddev(\"ArrDelay\").alias(\"StddevArrDelay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una función de Spark que hace casi todo esto por nosotros, llamada `summary`. Lo hace para cada columna numérica que encuentre en el dataset. Es también una transformación que devuelve un nuevo DataFrame con las métricas de resumen, sin modificar el DataFrame original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+----------+-------+------------+-------+------------+------------------+-----------------+------------------+------------------+---------+----------------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|summary|             Month|        DayofMonth|         DayOfWeek|FlightDate| Origin|  OriginCity|   Dest|    DestCity|           DepTime|         DepDelay|           ArrTime|          ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|           AirTime|         Distance|      CarrierDelay|      WeatherDelay|          NASDelay|      SecurityDelay| LateAircraftDelay|\n",
      "+-------+------------------+------------------+------------------+----------+-------+------------+-------+------------+------------------+-----------------+------------------+------------------+---------+----------------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|  count|           2196188|           2196188|           2196188|   2196188|2196188|     2196188|2196188|     2196188|           2196188|          2194296|           2196187|           2196188|  2196188|               0| 2196188|          2196186|           2193535|          2196188|            391787|            391787|            391787|             391787|            391787|\n",
      "|   mean|2.5424822465107724|15.649597393301484|3.9106383424369864|      null|   null|        null|   null|        null|1334.5825844599824|8.766424857904312|1474.2340406349733|3.2271057851149356|      0.0|            null|     0.0|133.3400144614345|108.50288324553746| 767.083472362111|20.047349707876982|3.3896428416460984|14.430899953290947|0.08252443291890747|25.278151138246038|\n",
      "| stddev|1.1247725681241427| 8.705057079326384|  2.00460284110096|      null|   null|        null|   null|        null|498.56585393237594|43.39281113609534| 527.5608038990398| 45.64141143439372|      0.0|            null|     0.0|72.04153459298358| 70.25405342616554|584.2715921760207| 60.29734847635768|30.546106131362542|32.590340292878096| 2.3974645711551097|49.937616798511314|\n",
      "|    min|                 1|                 1|                 1|2018-01-01|    ABE|Aberdeen, SD|    ABE|Aberdeen, SD|                 1|          -1280.0|                 1|           -1290.0|      0.0|            null|     0.0|          -1228.0|           -1244.0|             16.0|               0.0|               0.0|               0.0|                0.0|               0.0|\n",
      "|    25%|                 2|                 8|                 2|      null|   null|        null|   null|        null|               920|             -6.0|              1055|             -15.0|      0.0|            null|     0.0|             82.0|              58.0|            337.0|               0.0|               0.0|               0.0|                0.0|               0.0|\n",
      "|    50%|                 3|                16|                 4|      null|   null|        null|   null|        null|              1329|             -3.0|              1510|              -7.0|      0.0|            null|     0.0|            115.0|              90.0|            604.0|               1.0|               0.0|               2.0|                0.0|               2.0|\n",
      "|    75%|                 4|                23|                 6|      null|   null|        null|   null|        null|              1741|              6.0|              1914|               7.0|      0.0|            null|     0.0|            164.0|             138.0|           1009.0|              17.0|               0.0|              19.0|                0.0|              30.0|\n",
      "|    max|                 4|                31|                 7|2018-04-30|    YUM|    Yuma, AZ|    YUM|    Yuma, AZ|              2400|           2468.0|              2400|            2454.0|      0.0|            null|     0.0|            744.0|             696.0|           4983.0|            1752.0|            1682.0|            1495.0|              593.0|            2454.0|\n",
      "+-------+------------------+------------------+------------------+----------+-------+------------+-------+------------+------------------+-----------------+------------------+------------------+---------+----------------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summariesDF = flightsDF.summary()\n",
    "summariesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión a Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general es complicado leer los resultados que muestra Spark. En ocasiones interesa convertirlos a un dataframe de Pandas, aunque esto implica traer todas las filas al driver, lo cual **debe hacerse con mucho cuidado y solo en casos en los que estemos seguros de que el DataFrame de Spark es pequeño y no desbordará la memoria del driver**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><b>CONSEJO</b>: el concepto de dataframe como tabla con filas y columnas existe en muchos lenguajes de programación (pandas de Python, dataframes de R, DataFrames de Spark). Sin embargo, Spark maneja DataFrames que están físicamente distribuidos en las memorias RAM de las máquinas del cluster, lo cual no tiene nada que ver con lo que hace la biblioteca Pandas o R que se ejecutan en una sola máquina. Convertir un DataFrame de Spark en un dataframe de Pandas implica llevar todas las filas al driver, lo cual podría resultar en una excepción Out-of-Memory si el contenido del DataFrame es más grande que la memoria RAM de la máquina en la que se está ejecutando el programa driver. En este caso y en la mayoría de casos, se suele utilizar para mostrar resúmenes o agregados ya calculados previamente, y que sabemos que ocupan poco, con lo que no existe riesgo.</p>\n",
    "\n",
    "<p>Esta operación también es muy frecuente cuando queremos representar gráficamente el contenido de un DataFrame de Spark. No existen funciones gráficas en Spark, por lo que tenemos que convertirlo en un dataframe de Pandas y utilizar las funciones gráficas de Python habituales (matplotlib, Seaborn o incluso la propia biblioteca Pandas) para mostrar lo que necesitemos.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsPd = flightsDF.summary().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>FlightDate</th>\n",
       "      <th>Origin</th>\n",
       "      <th>OriginCity</th>\n",
       "      <th>Dest</th>\n",
       "      <th>DestCity</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>...</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>Distance</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196188</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2196188</td>\n",
       "      <td>2196186</td>\n",
       "      <td>2193535</td>\n",
       "      <td>2196188</td>\n",
       "      <td>391787</td>\n",
       "      <td>391787</td>\n",
       "      <td>391787</td>\n",
       "      <td>391787</td>\n",
       "      <td>391787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>2.5424822465107724</td>\n",
       "      <td>15.649597393301484</td>\n",
       "      <td>3.9106383424369864</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1334.5825844599824</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.3400144614345</td>\n",
       "      <td>108.50288324553746</td>\n",
       "      <td>767.083472362111</td>\n",
       "      <td>20.047349707876982</td>\n",
       "      <td>3.3896428416460984</td>\n",
       "      <td>14.430899953290947</td>\n",
       "      <td>0.08252443291890747</td>\n",
       "      <td>25.278151138246038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.1247725681241427</td>\n",
       "      <td>8.705057079326384</td>\n",
       "      <td>2.00460284110096</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>498.56585393237594</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.04153459298358</td>\n",
       "      <td>70.25405342616554</td>\n",
       "      <td>584.2715921760207</td>\n",
       "      <td>60.29734847635768</td>\n",
       "      <td>30.546106131362542</td>\n",
       "      <td>32.590340292878096</td>\n",
       "      <td>2.3974645711551097</td>\n",
       "      <td>49.937616798511314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>ABE</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>ABE</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1228.0</td>\n",
       "      <td>-1244.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25%</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50%</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1329</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75%</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1741</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>YUM</td>\n",
       "      <td>Yuma, AZ</td>\n",
       "      <td>YUM</td>\n",
       "      <td>Yuma, AZ</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>4983.0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>1495.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>2454.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary               Month          DayofMonth           DayOfWeek  \\\n",
       "0   count             2196188             2196188             2196188   \n",
       "1    mean  2.5424822465107724  15.649597393301484  3.9106383424369864   \n",
       "2  stddev  1.1247725681241427   8.705057079326384    2.00460284110096   \n",
       "3     min                   1                   1                   1   \n",
       "4     25%                   2                   8                   2   \n",
       "5     50%                   3                  16                   4   \n",
       "6     75%                   4                  23                   6   \n",
       "7     max                   4                  31                   7   \n",
       "\n",
       "   FlightDate   Origin    OriginCity     Dest      DestCity  \\\n",
       "0     2196188  2196188       2196188  2196188       2196188   \n",
       "1        None     None          None     None          None   \n",
       "2        None     None          None     None          None   \n",
       "3  2018-01-01      ABE  Aberdeen, SD      ABE  Aberdeen, SD   \n",
       "4        None     None          None     None          None   \n",
       "5        None     None          None     None          None   \n",
       "6        None     None          None     None          None   \n",
       "7  2018-04-30      YUM      Yuma, AZ      YUM      Yuma, AZ   \n",
       "\n",
       "              DepTime  ... CancellationCode Diverted  ActualElapsedTime  \\\n",
       "0             2196188  ...                0  2196188            2196186   \n",
       "1  1334.5825844599824  ...             None      0.0  133.3400144614345   \n",
       "2  498.56585393237594  ...             None      0.0  72.04153459298358   \n",
       "3                   1  ...             None      0.0            -1228.0   \n",
       "4                 920  ...             None      0.0               82.0   \n",
       "5                1329  ...             None      0.0              115.0   \n",
       "6                1741  ...             None      0.0              164.0   \n",
       "7                2400  ...             None      0.0              744.0   \n",
       "\n",
       "              AirTime           Distance        CarrierDelay  \\\n",
       "0             2193535            2196188              391787   \n",
       "1  108.50288324553746   767.083472362111  20.047349707876982   \n",
       "2   70.25405342616554  584.2715921760207   60.29734847635768   \n",
       "3             -1244.0               16.0                 0.0   \n",
       "4                58.0              337.0                 0.0   \n",
       "5                90.0              604.0                 1.0   \n",
       "6               138.0             1009.0                17.0   \n",
       "7               696.0             4983.0              1752.0   \n",
       "\n",
       "         WeatherDelay            NASDelay        SecurityDelay  \\\n",
       "0              391787              391787               391787   \n",
       "1  3.3896428416460984  14.430899953290947  0.08252443291890747   \n",
       "2  30.546106131362542  32.590340292878096   2.3974645711551097   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "4                 0.0                 0.0                  0.0   \n",
       "5                 0.0                 2.0                  0.0   \n",
       "6                 0.0                19.0                  0.0   \n",
       "7              1682.0              1495.0                593.0   \n",
       "\n",
       "    LateAircraftDelay  \n",
       "0              391787  \n",
       "1  25.278151138246038  \n",
       "2  49.937616798511314  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 2.0  \n",
       "6                30.0  \n",
       "7              2454.0  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsPd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra:\n",
    "    Crear el esquema de los datos y leerlos posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "esquema = StructType([ StructField(\"Month\",IntegerType(),True),\n",
    "                    StructField(\"DayofMonth\",IntegerType(),True),\n",
    "                    StructField(\"DayOfWeek\",IntegerType(),True),\n",
    "                    StructField(\"FlightDate\",StringType() ,True),\n",
    "                    StructField(\"Origin\",StringType() ,True),\n",
    "                    StructField(\"OriginCity\",StringType() ,True),\n",
    "                    StructField(\"Dest\",StringType() ,True),\n",
    "                    StructField(\"DestCity\",StringType() ,True),\n",
    "                    StructField(\"DepTime\", StringType(),True),\n",
    "                    StructField(\"DepDelay\", IntegerType(),True),\n",
    "                    StructField(\"ArrTime\", StringType(),True),\n",
    "                    StructField(\"ArrDelay\", IntegerType(),True),\n",
    "                    StructField(\"Cancelled\",BooleanType() ,True),\n",
    "                    StructField(\"CancellationCode\",StringType() ,True),\n",
    "                    StructField(\"Diverted\",BooleanType() ,True),\n",
    "                    StructField(\"ActualElapsedTime\",IntegerType() ,True),\n",
    "                    StructField(\"AirTime\", IntegerType(),True),\n",
    "                    StructField(\"Distance\", IntegerType(),True),\n",
    "                    StructField(\"CarrierDelay\",IntegerType() ,True),\n",
    "                    StructField(\"WeatherDelay\",IntegerType() ,True),\n",
    "                    StructField(\"NASDelay\",IntegerType() ,True),\n",
    "                    StructField(\"SecurityDelay\",IntegerType() ,True),\n",
    "                    StructField(\"LateAircraftDelay\",IntegerType() ,True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfflights= \"C:/Users/nerea.gomez/Documents/Documentacion/Ejercicios DataFrames/flights-jan-apr-2018.csv\"\n",
    "flightsDF = spark.read.csv(dfflights, header=True, schema=esquema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- Cancelled: boolean (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: boolean (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: integer (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
